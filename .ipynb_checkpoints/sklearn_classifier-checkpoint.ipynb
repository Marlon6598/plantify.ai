{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c81aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test run to see if we can create an image classifier using Sci-kit Learn instead of Tensorflow.\n",
    "# Following tutorials found here:\n",
    "#    https://kapernikov.com/tutorial-image-classification-with-scikit-learn/\n",
    "#    https://www.youtube.com/watch?v=il8dMDlXrIE\n",
    "\n",
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Had to look this one up. Apparently it's for quick pipelining?\n",
    "import joblib\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07590cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to grab the train/test data and store them as numpy arrays... in a pickle file. \n",
    "# I'm not sure I fully understand the purpose of a pickle file.\n",
    "\n",
    "def resize_images(path,pickle_name,include,width=150,height=150):\n",
    "    '''\n",
    "    The goal: load in all the images from the path, resize them, and write them as arrays to a dictionary with labels.\n",
    "    Save that dictionary to a pickle file.\n",
    "    I picked width 150 just because the Tensorflow classifier used that sizing.\n",
    "    \n",
    "    The parameters:\n",
    "    path: the file path where the images are stored. \n",
    "        Hopefully I can make this work with a link to the github repository at some point.\n",
    "    pickle_name: a string that becomes part of the file name for the new pickle file\n",
    "    width: target width of the image in pixels, default 150\n",
    "    height: target width of the image in pixels, default 150\n",
    "    include: set[str] = a set containing strings. These strings should be the names of the subdirectories in that path location.\n",
    "    '''\n",
    "    \n",
    "    # Make a dictionary, since I've got to write one to a file eventually.\n",
    "    data = dict()\n",
    "    data['label']=[]\n",
    "    data['filename']=[]\n",
    "    data['data']=[]\n",
    "    \n",
    "    # Make up a file name for the upcoming pickle file.\n",
    "    pickle_file_name = f\"{pickle_name}_{width}x{height}px.pkl\"\n",
    "    \n",
    "    # Read all the images in the path \n",
    "    for subdirectory in os.listdir(path):\n",
    "        # Have to start by making it through all the subdirectories in the path.\n",
    "        # I intend to use this by setting my paths to:\n",
    "        #    \"images/train\" and \"images/test\"\n",
    "        # so all the subdirectories will be the particular types of plants.\n",
    "        if subdirectory in include:\n",
    "            current_path = os.path.join(path,subdirectory)\n",
    "            \n",
    "            for image_file in os.listdir(current_path):\n",
    "                # I happen to know that all the images in the dataset are PNG files, so I'm not going to check for file type.\n",
    "                # Nicely resize the image using the tools imported from SKLearn\n",
    "                image = imread(os.path.join(current_path,image_file))\n",
    "                resized_image = resize(image,(width,height))\n",
    "                \n",
    "                # It's time to toss the info in our dictionary.\n",
    "                data['label'].append(subdirectory) # since our plant label is in the folder name\n",
    "                data['filename'].append(image_file)\n",
    "                data['data'].append(resized_image)\n",
    "    \n",
    "    # Throw that data in a pickle file.\n",
    "    joblib.dump(data,pickle_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199166da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time to get resizing. First up, the training set.\n",
    "# Let's find the labeled set first.\n",
    "train_path = \"images/train\"\n",
    "# Store the names of all the subdirectories in there. \n",
    "# This will become our include list, since I don't want to exclude any plant types.\n",
    "train_subdir = os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d5b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try using that function... I'll leave the default values for height and width.\n",
    "resize_images(path=train_path,pickle_name=\"training_set\",include=train_subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881d08f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4750\n",
      "Keys: dict_keys(['label', 'filename', 'data'])\n",
      "Labels: ['Black-grass' 'Charlock' 'Cleavers' 'Common Chickweed' 'Common wheat'\n",
      " 'Fat Hen' 'Loose Silky-bent' 'Maize' 'Scentless Mayweed'\n",
      " 'Shepherds Purse' 'Small-flowered Cranesbill' 'Sugar beet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'Black-grass': 263,\n",
       "         'Charlock': 390,\n",
       "         'Cleavers': 287,\n",
       "         'Common Chickweed': 611,\n",
       "         'Common wheat': 221,\n",
       "         'Fat Hen': 475,\n",
       "         'Loose Silky-bent': 654,\n",
       "         'Maize': 221,\n",
       "         'Scentless Mayweed': 516,\n",
       "         'Shepherds Purse': 231,\n",
       "         'Small-flowered Cranesbill': 496,\n",
       "         'Sugar beet': 385})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if it worked.\n",
    "check = joblib.load(\"training_set_150x150px.pkl\")\n",
    "\n",
    "print(f\"Number of samples: {len(check['data'])}\")\n",
    "print(f\"Keys: {check.keys()}\")\n",
    "print(f\"Labels: {np.unique(check['label'])}\")\n",
    "\n",
    "Counter(check['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b1a9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanmi\\AppData\\Local\\Temp\\ipykernel_24292\\1104086892.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.array(check['data'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (150,150,3) into shape (150,150)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24292\\1104086892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# That worked. I happen to know there are 4750 images in the labeled dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Let's grab the data and labels, then we'll have to split them up into training and testing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (150,150,3) into shape (150,150)"
     ]
    }
   ],
   "source": [
    "# That worked. I happen to know there are 4750 images in the labeled dataset.\n",
    "# Let's grab the data and labels, then we'll have to split them up into training and testing.\n",
    "data = np.array(check['data'])\n",
    "labels = np.array(check['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c977ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm leaving this error here instead of trying to fix it and go back because... it's an educational experience.\n",
    "# I don't know why the data array is shaped this way, I'm just gonna try to work around it.\n",
    "# Let's try another transformation - flattening. Vectors are a lot easier to work with.\n",
    "data = np.array([i.flatten() for i in check['data']],dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ec8b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(check['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca8702cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time to pray... or split into training and testing sets.\n",
    "x_train,x_test,y_train,y_test = train_test_split(data,labels,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b271a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a classifier and a whole bunch of parameter options, for curiosity's sake.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "\n",
    "parameters = [{'C':[1,10,100],'kernel':['linear','poly','rbf','sigmoid'],'gamma':[0.01,0.001,0.0001]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46b59598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(classifier, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "086d9a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "180 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "TypeError: only size-1 arrays can be converted to Python scalars\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 190, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 746, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "ValueError: setting an array element with a sequence.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24292\\956851232.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit the model(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    191\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Fit the model(s).\n",
    "grid_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460db44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
