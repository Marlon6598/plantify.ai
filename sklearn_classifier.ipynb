{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c81aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test run to see if we can create an image classifier using Sci-kit Learn instead of Tensorflow.\n",
    "# Following tutorial found here:\n",
    "#    https://kapernikov.com/tutorial-image-classification-with-scikit-learn/\n",
    "\n",
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Had to look this one up. Apparently it's for quick pipelining?\n",
    "import joblib\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07590cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to grab the train/test data and store them as numpy arrays... in a pickle file. \n",
    "# I'm not sure I fully understand the purpose of a pickle file.\n",
    "\n",
    "def resize_images(path,pickle_name,include,width=150,height=150):\n",
    "    '''\n",
    "    The goal: load in all the images from the path, resize them, and write them as arrays to a dictionary with labels.\n",
    "    Save that dictionary to a pickle file.\n",
    "    I picked width 150 just because the Tensorflow classifier used that sizing.\n",
    "    \n",
    "    The parameters:\n",
    "    path: the file path where the images are stored. \n",
    "        Hopefully I can make this work with a link to the github repository at some point.\n",
    "    pickle_name: a string that becomes part of the file name for the new pickle file\n",
    "    width: target width of the image in pixels, default 150\n",
    "    height: target width of the image in pixels, default 150\n",
    "    include: set[str] = a set containing strings. These strings should be the names of the subdirectories in that path location.\n",
    "    '''\n",
    "    \n",
    "    # Make a dictionary, since I've got to write one to a file eventually.\n",
    "    data = dict()\n",
    "    data['label']=[]\n",
    "    data['filename']=[]\n",
    "    data['data']=[]\n",
    "    \n",
    "    # Make up a file name for the upcoming pickle file.\n",
    "    pickle_file_name = f\"{pickle_name}_{width}x{height}px.pkl\"\n",
    "    \n",
    "    # Read all the images in the path \n",
    "    for subdirectory in os.listdir(path):\n",
    "        # Have to start by making it through all the subdirectories in the path.\n",
    "        # I intend to use this by setting my paths to:\n",
    "        #    \"images/train\" and \"images/test\"\n",
    "        # so all the subdirectories will be the particular types of plants.\n",
    "        if subdirectory in include:\n",
    "            current_path = os.path.join(path,subdirectory)\n",
    "            \n",
    "            for image_file in os.listdir(current_path):\n",
    "                # I happen to know that all the images in the dataset are PNG files, so I'm not going to check for file type.\n",
    "                # Nicely resize the image using the tools imported from SKLearn\n",
    "                image = imread(os.path.join(current_path,image_file))\n",
    "                resized_image = resize(image,(width,height))\n",
    "                \n",
    "                # It's time to toss the info in our dictionary.\n",
    "                data['label'].append(subdirectory) # since our plant label is in the folder name\n",
    "                data['filename'].append(image_file)\n",
    "                data['data'].append(resized_image)\n",
    "    \n",
    "    # Throw that data in a pickle file.\n",
    "    joblib.dump(data,pickle_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199166da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time to get resizing. First up, the training set.\n",
    "# Let's find the labeled set first.\n",
    "train_path = \"images/train\"\n",
    "# Store the names of all the subdirectories in there. \n",
    "# This will become our include list, since I don't want to exclude any plant types.\n",
    "train_subdir = os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try using that function... I'll leave the default values for height and width.\n",
    "resize_images(path=train_path,pickle_name=\"training_set\",include=train_subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881d08f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4750\n",
      "Keys: dict_keys(['label', 'filename', 'data'])\n",
      "Labels: ['Black-grass' 'Charlock' 'Cleavers' 'Common Chickweed' 'Common wheat'\n",
      " 'Fat Hen' 'Loose Silky-bent' 'Maize' 'Scentless Mayweed'\n",
      " 'Shepherds Purse' 'Small-flowered Cranesbill' 'Sugar beet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'Black-grass': 263,\n",
       "         'Charlock': 390,\n",
       "         'Cleavers': 287,\n",
       "         'Common Chickweed': 611,\n",
       "         'Common wheat': 221,\n",
       "         'Fat Hen': 475,\n",
       "         'Loose Silky-bent': 654,\n",
       "         'Maize': 221,\n",
       "         'Scentless Mayweed': 516,\n",
       "         'Shepherds Purse': 231,\n",
       "         'Small-flowered Cranesbill': 496,\n",
       "         'Sugar beet': 385})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if it worked.\n",
    "check = joblib.load(\"training_set_150x150px.pkl\")\n",
    "\n",
    "print(f\"Number of samples: {len(check['data'])}\")\n",
    "print(f\"Keys: {check.keys()}\")\n",
    "print(f\"Labels: {np.unique(check['label'])}\")\n",
    "\n",
    "Counter(check['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11108d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanmi\\AppData\\Local\\Temp\\ipykernel_832\\311942066.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.array(check['data'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (150,150,3) into shape (150,150)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_832\\311942066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (150,150,3) into shape (150,150)"
     ]
    }
   ],
   "source": [
    "data = np.array(check['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b1e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm leaving this error here because... it's an educational experience.\n",
    "shapes = [i.shape for i in check['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40d4dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(150, 150, 3), (150, 150, 4)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I expect only to have one shape, so then just one element in this set.\n",
    "set(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf55aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so my data shapes are not consistent... I don't love that\n",
    "# But I also wonder if I can get away with just resizing every entry.\n",
    "data = [np.resize(i,(150,150,4)).flatten() for i in check['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a40c1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(90000,)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check my array shapes again.\n",
    "shapes = [i.shape for i in data]\n",
    "set(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf18b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(check['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dea925a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4750"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the length of each of those arrays\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efe82503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4750"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b5d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# They match up, nice. \n",
    "# The data now has to be split into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,labels,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71aacc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tutorial uses a few different transformations.\n",
    "# This other one just goes straight to a support vector classification. I want to see what happens there.\n",
    "classifier = SVC()\n",
    "\n",
    "# Try out a whole bunch of parameters and see what happens.\n",
    "parameters = [{'gamma': [0.01, 0.001], 'C': [1, 10], 'kernel':['poly','rbf']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier (or rather, the classifiers)\n",
    "grid_search = GridSearchCV(classifier, parameters)\n",
    "\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df913f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
